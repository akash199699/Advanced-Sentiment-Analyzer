{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvwdzhM6W4Nk"
      },
      "source": [
        "# **Sentiment Analyzer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5T_yC_0lZDq",
        "outputId": "f1d07b5b-d7a7-45bd-bda9-da00115750f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b7676fe1e98f6d1dc9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b7676fe1e98f6d1dc9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"Advanced Sentiment Analysis System with Gradio Interface\n",
        "Supports both traditional ML (TF-IDF + SVM) and Transformer-based approaches\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import gradio as gr\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import base64\n",
        "\n",
        "# Traditional ML imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# NLP preprocessing imports\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Transformer imports\n",
        "try:\n",
        "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Transformers not available. Install with: pip install transformers torch\")\n",
        "\n",
        "# Download required NLTK data\n",
        "def download_nltk_data():\n",
        "    \"\"\"Download all required NLTK data\"\"\"\n",
        "    required_data = [\n",
        "        ('tokenizers/punkt', 'punkt'),\n",
        "        ('tokenizers/punkt_tab', 'punkt_tab'),\n",
        "        ('corpora/stopwords', 'stopwords'),\n",
        "        ('corpora/wordnet', 'wordnet'),\n",
        "        ('corpora/omw-1.4', 'omw-1.4'),\n",
        "        ('vader_lexicon', 'vader_lexicon')\n",
        "    ]\n",
        "\n",
        "    for data_path, data_name in required_data:\n",
        "        try:\n",
        "            nltk.data.find(data_path)\n",
        "        except LookupError:\n",
        "            try:\n",
        "                nltk.download(data_name, quiet=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not download {data_name}: {e}\")\n",
        "\n",
        "# Download NLTK data\n",
        "download_nltk_data()\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Handle text preprocessing tasks\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        except LookupError:\n",
        "            # Fallback stopwords if NLTK data not available\n",
        "            self.stop_words = {\n",
        "                'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
        "                'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
        "                'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
        "                'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
        "                'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n",
        "                'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
        "                'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after', 'above',\n",
        "                'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "                'further', 'then', 'once'\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "        except:\n",
        "            self.lemmatizer = None\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove user mentions and hashtags\n",
        "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "        # Remove extra whitespace and newlines\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters but keep basic punctuation\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\!\\?]', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def preprocess_text(self, text: str, remove_stopwords: bool = True,\n",
        "                       lemmatize: bool = True) -> str:\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        # Clean text\n",
        "        text = self.clean_text(text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Tokenize - handle different NLTK versions\n",
        "        try:\n",
        "            tokens = word_tokenize(text)\n",
        "        except LookupError:\n",
        "            # Fallback to simple split if tokenizer not available\n",
        "            tokens = text.split()\n",
        "\n",
        "        # Remove stopwords\n",
        "        if remove_stopwords:\n",
        "            tokens = [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
        "\n",
        "        # Lemmatize\n",
        "        if lemmatize and self.lemmatizer:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    \"\"\"Main sentiment analysis class supporting multiple approaches\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.traditional_model = None\n",
        "        self.transformer_model = None\n",
        "        self.model_type = None\n",
        "        self.label_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "        self.reverse_label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "\n",
        "    def load_custom_data(self, file) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Load custom dataset from uploaded file\"\"\"\n",
        "        try:\n",
        "            # Handle different file input types from Gradio\n",
        "            if hasattr(file, 'name'):\n",
        "                file_path = str(file)\n",
        "            else:\n",
        "                file_path = file\n",
        "\n",
        "            # Try to read CSV file\n",
        "            try:\n",
        "                df = pd.read_csv(file_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading CSV: {e}\")\n",
        "                return None\n",
        "\n",
        "            # Validate required columns\n",
        "            required_columns = ['text', 'sentiment']\n",
        "            if not all(col in df.columns for col in required_columns):\n",
        "                # Try common column name variations\n",
        "                column_mappings = {\n",
        "                    'review': 'text',\n",
        "                    'comment': 'text',\n",
        "                    'message': 'text',\n",
        "                    'content': 'text',\n",
        "                    'label': 'sentiment',\n",
        "                    'class': 'sentiment',\n",
        "                    'category': 'sentiment',\n",
        "                    'rating': 'sentiment'\n",
        "                }\n",
        "\n",
        "                # Apply mappings\n",
        "                df = df.rename(columns=column_mappings)\n",
        "\n",
        "                # Check again\n",
        "                if not all(col in df.columns for col in required_columns):\n",
        "                    available_cols = list(df.columns)\n",
        "                    print(f\"Required columns 'text' and 'sentiment' not found. Available columns: {available_cols}\")\n",
        "                    return None\n",
        "\n",
        "            # Clean and validate data\n",
        "            df = df.dropna(subset=['text', 'sentiment'])\n",
        "            df['text'] = df['text'].astype(str)\n",
        "            df['sentiment'] = df['sentiment'].astype(str)\n",
        "\n",
        "            # Standardize sentiment labels\n",
        "            df['sentiment'] = df['sentiment'].apply(self._standardize_sentiment_label)\n",
        "\n",
        "            # Remove any rows with unrecognized sentiment labels\n",
        "            valid_labels = {'Positive', 'Negative', 'Neutral'}\n",
        "            df = df[df['sentiment'].isin(valid_labels)]\n",
        "\n",
        "            if len(df) == 0:\n",
        "                print(\"No valid data rows found after processing.\")\n",
        "                return None\n",
        "\n",
        "            print(f\"Successfully loaded {len(df)} rows of custom data.\")\n",
        "            print(f\"Sentiment distribution: {df['sentiment'].value_counts().to_dict()}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading custom data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _standardize_sentiment_label(self, label: str) -> str:\n",
        "        \"\"\"Standardize sentiment labels to Positive/Negative/Neutral\"\"\"\n",
        "        label = str(label).lower().strip()\n",
        "\n",
        "        # Positive indicators\n",
        "        if label in ['positive', 'pos', '1', 'good', 'happy', 'like', 'love', '4', '5']:\n",
        "            return 'Positive'\n",
        "\n",
        "        # Negative indicators\n",
        "        elif label in ['negative', 'neg', '0', 'bad', 'sad', 'hate', 'dislike', '1', '2']:\n",
        "            return 'Negative'\n",
        "\n",
        "        # Neutral indicators\n",
        "        elif label in ['neutral', 'neu', '2', 'okay', 'ok', 'average', 'mixed', '3']:\n",
        "            return 'Neutral'\n",
        "\n",
        "        # Try to handle numeric ratings (assuming 1-5 scale)\n",
        "        try:\n",
        "            rating = float(label)\n",
        "            if rating >= 4:\n",
        "                return 'Positive'\n",
        "            elif rating <= 2:\n",
        "                return 'Negative'\n",
        "            else:\n",
        "                return 'Neutral'\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Default to neutral for unrecognized labels\n",
        "        return 'Neutral'\n",
        "\n",
        "    def load_sample_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Create sample dataset for demonstration\"\"\"\n",
        "        sample_data = [\n",
        "            (\"I love this movie! It's absolutely fantastic.\", \"Positive\"),\n",
        "            (\"This film is terrible. Waste of time.\", \"Negative\"),\n",
        "            (\"The movie was okay, nothing special.\", \"Neutral\"),\n",
        "            (\"Amazing acting and great storyline!\", \"Positive\"),\n",
        "            (\"Boring and predictable plot.\", \"Negative\"),\n",
        "            (\"The film has its moments but overall average.\", \"Neutral\"),\n",
        "            (\"Brilliant cinematography and outstanding performances!\", \"Positive\"),\n",
        "            (\"Poor direction and weak script.\", \"Negative\"),\n",
        "            (\"It's an acceptable movie for passing time.\", \"Neutral\"),\n",
        "            (\"Masterpiece! One of the best films ever made.\", \"Positive\"),\n",
        "            (\"Completely disappointed with this movie.\", \"Negative\"),\n",
        "            (\"The movie is fine, not bad but not great either.\", \"Neutral\"),\n",
        "            (\"Exceptional storytelling and character development.\", \"Positive\"),\n",
        "            (\"The worst movie I've ever seen.\", \"Negative\"),\n",
        "            (\"Standard Hollywood production, nothing remarkable.\", \"Neutral\"),\n",
        "            (\"This movie changed my perspective on life!\", \"Positive\"),\n",
        "            (\"Overrated and overhyped garbage.\", \"Negative\"),\n",
        "            (\"Decent enough for a single watch.\", \"Neutral\"),\n",
        "            (\"Incredible visual effects and sound design.\", \"Positive\"),\n",
        "            (\"Confusing plot and poor character development.\", \"Negative\")\n",
        "        ]\n",
        "\n",
        "        return pd.DataFrame(sample_data, columns=['text', 'sentiment'])\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame) -> Tuple[List[str], List[int]]:\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        # Preprocess texts\n",
        "        processed_texts = [self.preprocessor.preprocess_text(text) for text in df['text']]\n",
        "\n",
        "        # Convert labels to numeric\n",
        "        labels = [self.reverse_label_map.get(label, 1) for label in df['sentiment']]\n",
        "\n",
        "        return processed_texts, labels\n",
        "\n",
        "    def train_traditional_model(self, df: pd.DataFrame, model_type: str = 'svm') -> Dict:\n",
        "        \"\"\"Train traditional ML model with TF-IDF features\"\"\"\n",
        "        texts, labels = self.prepare_data(df)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "\n",
        "        # Choose model\n",
        "        if model_type == 'svm':\n",
        "            model = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
        "        elif model_type == 'rf':\n",
        "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        else:\n",
        "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "        # Create pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
        "            ('classifier', model)\n",
        "        ])\n",
        "\n",
        "        # Train model\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        metrics = self._calculate_metrics(y_test, y_pred)\n",
        "\n",
        "        self.traditional_model = pipeline\n",
        "        self.model_type = 'traditional'\n",
        "\n",
        "        return {\n",
        "            'model': pipeline,\n",
        "            'metrics': metrics,\n",
        "            'test_texts': X_test,\n",
        "            'y_true': y_test,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "    def load_transformer_model(self, model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n",
        "        \"\"\"Load pre-trained transformer model\"\"\"\n",
        "        if not TRANSFORMERS_AVAILABLE:\n",
        "            raise ImportError(\"Transformers library not available\")\n",
        "\n",
        "        try:\n",
        "            self.transformer_model = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=model_name,\n",
        "                tokenizer=model_name,\n",
        "                return_all_scores=True\n",
        "            )\n",
        "            self.model_type = 'transformer'\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading transformer model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _calculate_metrics(self, y_true: List[int], y_pred: List[int]) -> Dict:\n",
        "        \"\"\"Calculate performance metrics\"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "\n",
        "    def predict_traditional(self, text: str) -> Dict:\n",
        "        \"\"\"Predict using traditional model\"\"\"\n",
        "        if self.traditional_model is None:\n",
        "            return {\"error\": \"Traditional model not trained\"}\n",
        "\n",
        "        processed_text = self.preprocessor.preprocess_text(text)\n",
        "        prediction = self.traditional_model.predict([processed_text])[0]\n",
        "        probabilities = self.traditional_model.predict_proba([processed_text])[0]\n",
        "\n",
        "        return {\n",
        "            'prediction': self.label_map[prediction],\n",
        "            'confidence': float(max(probabilities)),\n",
        "            'probabilities': {\n",
        "                self.label_map[i]: float(prob)\n",
        "                for i, prob in enumerate(probabilities)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def predict_transformer(self, text: str) -> Dict:\n",
        "        \"\"\"Predict using transformer model\"\"\"\n",
        "        if self.transformer_model is None:\n",
        "            return {\"error\": \"Transformer model not loaded\"}\n",
        "\n",
        "        try:\n",
        "            results = self.transformer_model(text)[0]\n",
        "\n",
        "            # Map transformer labels to our format\n",
        "            label_mapping = {\n",
        "                'LABEL_0': 'Negative', 'NEGATIVE': 'Negative',\n",
        "                'LABEL_1': 'Neutral', 'NEUTRAL': 'Neutral',\n",
        "                'LABEL_2': 'Positive', 'POSITIVE': 'Positive'\n",
        "            }\n",
        "\n",
        "            mapped_results = []\n",
        "            for result in results:\n",
        "                label = label_mapping.get(result['label'], result['label'])\n",
        "                mapped_results.append({'label': label, 'score': result['score']})\n",
        "\n",
        "            # Find best prediction\n",
        "            best_result = max(mapped_results, key=lambda x: x['score'])\n",
        "\n",
        "            return {\n",
        "                'prediction': best_result['label'],\n",
        "                'confidence': float(best_result['score']),\n",
        "                'probabilities': {r['label']: float(r['score']) for r in mapped_results}\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": f\"Prediction error: {str(e)}\"}\n",
        "\n",
        "    def predict(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment using the active model\"\"\"\n",
        "        if self.model_type == 'traditional':\n",
        "            return self.predict_traditional(text)\n",
        "        elif self.model_type == 'transformer':\n",
        "            return self.predict_transformer(text)\n",
        "        else:\n",
        "            return {\"error\": \"No model loaded\"}\n",
        "\n",
        "    def analyze_file(self, file_path: str) -> List[Dict]:\n",
        "        \"\"\"Analyze sentiment for text file\"\"\"\n",
        "        try:\n",
        "            # Handle different input types\n",
        "            if hasattr(file_path, 'name'):\n",
        "                # Gradio file object\n",
        "                actual_path = str(file_path)\n",
        "            else:\n",
        "                actual_path = file_path\n",
        "\n",
        "            with open(actual_path, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            results = []\n",
        "            for i, line in enumerate(lines):\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    result = self.predict(line)\n",
        "                    result['line_number'] = i + 1\n",
        "                    result['text'] = line\n",
        "                    results.append(result)\n",
        "\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            return [{\"error\": f\"File processing error: {str(e)}\"}]\n",
        "\n",
        "# Global analyzer instance\n",
        "analyzer = SentimentAnalyzer()\n",
        "\n",
        "def train_model(data_source: str, model_type: str, custom_file=None) -> str:\n",
        "    \"\"\"Train the sentiment analysis model\"\"\"\n",
        "    try:\n",
        "        # Load data based on source\n",
        "        if data_source == \"Sample Data\":\n",
        "            df = analyzer.load_sample_data()\n",
        "            data_info = f\"Using sample data with {len(df)} examples.\"\n",
        "        else:\n",
        "            if custom_file is None:\n",
        "                return \"Please upload a CSV file for custom data training.\"\n",
        "\n",
        "            # Load custom data\n",
        "            df = analyzer.load_custom_data(custom_file)\n",
        "            if df is None:\n",
        "                return \"Failed to load custom data. Please check your CSV format and ensure it has 'text' and 'sentiment' columns.\"\n",
        "\n",
        "            data_info = f\"Successfully loaded custom data with {len(df)} examples.\"\n",
        "\n",
        "        # Show data distribution\n",
        "        sentiment_dist = df['sentiment'].value_counts().to_dict()\n",
        "        dist_info = \"\\nSentiment Distribution:\\n\" + \"\\n\".join([f\"- {k}: {v}\" for k, v in sentiment_dist.items()])\n",
        "\n",
        "        if model_type == \"Traditional ML (TF-IDF + SVM)\":\n",
        "            result = analyzer.train_traditional_model(df, 'svm')\n",
        "            metrics = result['metrics']\n",
        "            return f\"\"\"{data_info}{dist_info}\n",
        "\n",
        "Model trained successfully!\n",
        "\n",
        "Performance Metrics:\n",
        "- Accuracy: {metrics['accuracy']:.3f}\n",
        "- Precision: {metrics['precision']:.3f}\n",
        "- Recall: {metrics['recall']:.3f}\n",
        "- F1-Score: {metrics['f1_score']:.3f}\n",
        "\n",
        "Model ready for predictions.\"\"\"\n",
        "\n",
        "        elif model_type == \"Transformer (BERT-based)\":\n",
        "            if analyzer.load_transformer_model():\n",
        "                return f\"\"\"{data_info}{dist_info}\n",
        "\n",
        "Transformer model loaded successfully!\n",
        "\n",
        "Note: Pre-trained transformer models don't require training on your specific data,\n",
        "but they provide state-of-the-art performance out of the box.\n",
        "\n",
        "Model ready for predictions.\"\"\"\n",
        "            else:\n",
        "                return \"Failed to load transformer model. Please check your internet connection or try the Traditional ML option.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Training failed: {str(e)}\\n\\nPlease check your data format and try again.\"\n",
        "\n",
        "def predict_sentiment(text: str) -> str:\n",
        "    \"\"\"Predict sentiment for input text\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to analyze.\"\n",
        "\n",
        "    # Check if any model is loaded\n",
        "    if analyzer.model_type is None:\n",
        "        return \"No model is currently loaded. Please train a model first in the 'Model Training' tab.\"\n",
        "\n",
        "    try:\n",
        "        result = analyzer.predict(text)\n",
        "\n",
        "        if 'error' in result:\n",
        "            return f\"Error: {result['error']}\"\n",
        "\n",
        "        output = f\"\"\"Prediction: {result['prediction']}\n",
        "Confidence: {result['confidence']:.3f}\n",
        "\n",
        "Probability Scores:\"\"\"\n",
        "\n",
        "        for label, prob in result['probabilities'].items():\n",
        "            output += f\"\\n- {label}: {prob:.3f}\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Prediction failed: {str(e)}\"\n",
        "\n",
        "def analyze_file_content(file) -> str:\n",
        "    \"\"\"Analyze sentiment for uploaded file\"\"\"\n",
        "    if file is None:\n",
        "        return \"Please upload a text file.\"\n",
        "\n",
        "    # Check if any model is loaded\n",
        "    if analyzer.model_type is None:\n",
        "        return \"No model is currently loaded. Please train a model first in the 'Model Training' tab.\"\n",
        "\n",
        "    try:\n",
        "        # Handle different file input types from Gradio\n",
        "        if hasattr(file, 'name'):\n",
        "            # file is a file path string or NamedString\n",
        "            file_path = str(file)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "        else:\n",
        "            # file is already file content\n",
        "            content = str(file)\n",
        "\n",
        "        lines = content.split('\\n')\n",
        "\n",
        "        results = []\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                result = analyzer.predict(line)\n",
        "                if 'error' not in result:\n",
        "                    results.append({\n",
        "                        'line': i + 1,\n",
        "                        'text': line[:50] + \"...\" if len(line) > 50 else line,\n",
        "                        'sentiment': result['prediction'],\n",
        "                        'confidence': result['confidence']\n",
        "                    })\n",
        "\n",
        "        if not results:\n",
        "            return \"No valid text found in the file.\"\n",
        "\n",
        "        # Create summary\n",
        "        sentiment_counts = {}\n",
        "        total_confidence = 0\n",
        "\n",
        "        for result in results:\n",
        "            sentiment = result['sentiment']\n",
        "            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n",
        "            total_confidence += result['confidence']\n",
        "\n",
        "        avg_confidence = total_confidence / len(results)\n",
        "\n",
        "        output = f\"\"\"File Analysis Complete!\n",
        "\n",
        "Total lines analyzed: {len(results)}\n",
        "Average confidence: {avg_confidence:.3f}\n",
        "\n",
        "Sentiment Distribution:\"\"\"\n",
        "\n",
        "        for sentiment, count in sentiment_counts.items():\n",
        "            percentage = (count / len(results)) * 100\n",
        "            output += f\"\\n- {sentiment}: {count} ({percentage:.1f}%)\"\n",
        "\n",
        "        output += \"\\n\\nSample Results:\"\n",
        "        for result in results[:5]:  # Show first 5 results\n",
        "            output += f\"\\nLine {result['line']}: {result['text']} â†’ {result['sentiment']} ({result['confidence']:.3f})\"\n",
        "\n",
        "        if len(results) > 5:\n",
        "            output += f\"\\n... and {len(results) - 5} more lines.\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"File analysis failed: {str(e)}\"\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Create the Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Advanced Sentiment Analyzer\", theme=gr.themes.Soft()) as interface:\n",
        "        gr.Markdown(\"# ðŸŽ­ Advanced Sentiment Analysis System\")\n",
        "        gr.Markdown(\"Train models and analyze sentiment in text using traditional ML or transformer approaches.\")\n",
        "\n",
        "        with gr.Tab(\"Model Training\"):\n",
        "            gr.Markdown(\"## Train Your Sentiment Analysis Model\")\n",
        "\n",
        "            with gr.Row():\n",
        "                data_source = gr.Dropdown(\n",
        "                    choices=[\"Sample Data\", \"Upload Custom Data\"],\n",
        "                    value=\"Sample Data\",\n",
        "                    label=\"Data Source\"\n",
        "                )\n",
        "                model_type = gr.Dropdown(\n",
        "                    choices=[\"Traditional ML (TF-IDF + SVM)\", \"Transformer (BERT-based)\"],\n",
        "                    value=\"Traditional ML (TF-IDF + SVM)\",\n",
        "                    label=\"Model Type\"\n",
        "                )\n",
        "\n",
        "            # Custom data upload section\n",
        "            with gr.Row():\n",
        "                custom_data_file = gr.File(\n",
        "                    label=\"Upload Custom Dataset (CSV)\",\n",
        "                    file_types=[\".csv\"],\n",
        "                    type=\"filepath\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "            # Data format information\n",
        "            data_format_info = gr.Markdown(\n",
        "                \"\"\"\n",
        "                ### Custom Data Format Requirements:\n",
        "                Your CSV file should have these columns:\n",
        "                - **text** (or review/comment/message): The text to analyze\n",
        "                - **sentiment** (or label/class): The sentiment label\n",
        "\n",
        "                **Supported sentiment labels:**\n",
        "                - Text: \"Positive\", \"Negative\", \"Neutral\" (or \"pos\", \"neg\", \"neu\")\n",
        "                - Numeric: 1-5 scale (1-2=Negative, 3=Neutral, 4-5=Positive)\n",
        "                - Binary: 0=Negative, 1=Positive\n",
        "\n",
        "                **Example CSV format:**\n",
        "                ```\n",
        "                text,sentiment\n",
        "                \"I love this product!\",Positive\n",
        "                \"This is terrible\",Negative\n",
        "                \"It's okay\",Neutral\n",
        "                ```\n",
        "                \"\"\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            def update_visibility(data_source):\n",
        "                if data_source == \"Upload Custom Data\":\n",
        "                    return gr.update(visible=True), gr.update(visible=True)\n",
        "                else:\n",
        "                    return gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "            data_source.change(\n",
        "                fn=update_visibility,\n",
        "                inputs=data_source,\n",
        "                outputs=[custom_data_file, data_format_info]\n",
        "            )\n",
        "\n",
        "            train_btn = gr.Button(\"Train Model\", variant=\"primary\")\n",
        "            training_output = gr.Textbox(label=\"Training Results\", lines=10)\n",
        "\n",
        "            train_btn.click(\n",
        "                fn=train_model,\n",
        "                inputs=[data_source, model_type, custom_data_file],\n",
        "                outputs=training_output\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Text Analysis\"):\n",
        "            gr.Markdown(\"## Analyze Individual Text\")\n",
        "\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"Enter Text to Analyze\",\n",
        "                placeholder=\"Type your text here...\",\n",
        "                lines=3\n",
        "            )\n",
        "            analyze_btn = gr.Button(\"Analyze Sentiment\", variant=\"primary\")\n",
        "            text_output = gr.Textbox(label=\"Analysis Results\", lines=8)\n",
        "\n",
        "            analyze_btn.click(\n",
        "                fn=predict_sentiment,\n",
        "                inputs=text_input,\n",
        "                outputs=text_output\n",
        "            )\n",
        "\n",
        "            # Example inputs\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"I absolutely love this product! It exceeded all my expectations.\"],\n",
        "                    [\"This movie was terrible. Complete waste of time.\"],\n",
        "                    [\"The service was okay, nothing special but not bad either.\"],\n",
        "                    [\"Amazing customer support! They solved my problem quickly.\"],\n",
        "                    [\"I'm not sure how I feel about this new update.\"]\n",
        "                ],\n",
        "                inputs=text_input\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"File Analysis\"):\n",
        "            gr.Markdown(\"## Analyze Text Files\")\n",
        "            gr.Markdown(\"Upload a text file (.txt) with one sentence/review per line.\")\n",
        "\n",
        "            file_input = gr.File(\n",
        "                label=\"Upload Text File\",\n",
        "                file_types=[\".txt\"],\n",
        "                type=\"filepath\"  # This ensures we get the file path\n",
        "            )\n",
        "            file_btn = gr.Button(\"Analyze File\", variant=\"primary\")\n",
        "            file_output = gr.Textbox(label=\"File Analysis Results\", lines=15)\n",
        "\n",
        "            file_btn.click(\n",
        "                fn=analyze_file_content,\n",
        "                inputs=file_input,\n",
        "                outputs=file_output\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"About\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## About This Sentiment Analyzer\n",
        "\n",
        "            This advanced sentiment analysis system supports two approaches:\n",
        "\n",
        "            ### Traditional ML Approach\n",
        "            - **Features**: TF-IDF vectorization with n-grams\n",
        "            - **Model**: Support Vector Machine (SVM) with RBF kernel\n",
        "            - **Preprocessing**: NLTK-based text cleaning, tokenization, and lemmatization\n",
        "            - **Performance**: Fast inference, lightweight model\n",
        "\n",
        "            ### Transformer Approach\n",
        "            - **Model**: Pre-trained RoBERTa-based sentiment classifier\n",
        "            - **Features**: Contextual embeddings from transformer architecture\n",
        "            - **Performance**: Higher accuracy, slower inference\n",
        "\n",
        "            ### Features\n",
        "            - âœ… Text preprocessing with NLTK\n",
        "            - âœ… Multiple model support (SVM, Random Forest, Logistic Regression)\n",
        "            - âœ… Transformer integration with HuggingFace\n",
        "            - âœ… File batch processing\n",
        "            - âœ… Performance metrics reporting\n",
        "            - âœ… Confidence scores and probability distributions\n",
        "            - âœ… Modular, extensible codebase\n",
        "\n",
        "            ### Usage\n",
        "            1. **Train**: Select data source and model type, then click \"Train Model\"\n",
        "            2. **Analyze**: Enter text or upload files for sentiment analysis\n",
        "            3. **Results**: View predictions with confidence scores and probabilities\n",
        "            \"\"\")\n",
        "\n",
        "    return interface\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and launch the interface\n",
        "    interface = create_interface()\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        show_error=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}